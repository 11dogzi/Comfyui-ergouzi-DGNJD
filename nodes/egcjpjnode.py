from typing import Tuple, Dict, Any
import torch
from PIL import Image
import numpy as np
from torchvision import transforms

def tensor2pil(image):
    return Image.fromarray(np.clip(255. * image.cpu().numpy().squeeze(), 0, 255).astype(np.uint8))

def pil2tensor(image):
    return torch.from_numpy(np.array(image).astype(np.float32) / 255.0).unsqueeze(0)

class EGCJPJNode:
    def __init__(self):
        pass

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "è¾“å…¥åŸå›¾": ("IMAGE",),
                "è¾“å…¥è£å‰ªå›¾åƒ": ("IMAGE",),
                "è¾“å…¥è£å‰ªæ•°æ®": ("COORDS",),
            },
        }

    RETURN_TYPES = ("IMAGE",)
    RETURN_NAMES = ("æ‹¼æ¥ç»“æœå›¾",)
    FUNCTION = "resize_and_paste"
    CATEGORY = "2ğŸ•/é®ç½©/ç»†åŒ–å¤„ç†"

    def resize_and_paste(self, è¾“å…¥åŸå›¾, è¾“å…¥è£å‰ªå›¾åƒ, è¾“å…¥è£å‰ªæ•°æ®):
        original_image_pil = tensor2pil(è¾“å…¥åŸå›¾)
        cropped_image_pil = tensor2pil(è¾“å…¥è£å‰ªå›¾åƒ)

        if è¾“å…¥è£å‰ªæ•°æ® is None:
            return (è¾“å…¥åŸå›¾,)

        
        y0, y1, x0, x1 = è¾“å…¥è£å‰ªæ•°æ®

        target_width = x1 - x0
        target_height = y1 - y0

        cropped_image_pil = cropped_image_pil.resize((target_width, target_height))

        original_image_pil.paste(cropped_image_pil, (x0, y0))

        pasted_image_tensor = pil2tensor(original_image_pil)

        return (pasted_image_tensor,)
# æœ¬å¥—æ’ä»¶ç‰ˆæƒæ‰€å±Bç«™@çµä»™å„¿å’ŒäºŒç‹—å­ï¼Œä»…ä¾›å­¦ä¹ äº¤æµä½¿ç”¨ï¼Œæœªç»æˆæƒç¦æ­¢ä¸€åˆ‡å•†ä¸šæ€§è´¨ä½¿ç”¨
